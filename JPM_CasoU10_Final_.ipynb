{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1 - Importación de librerías"
      ],
      "metadata": {
        "id": "8imOqBhI6LcH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waKoyPKmV6-P"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from matplotlib import pyplot as plt #Ploting charts\n",
        "from glob import glob #retriving an array of files in directories\n",
        "from keras.models import Sequential #for neural network models\n",
        "from keras.layers import Dense, Dropout, Flatten, ZeroPadding2D, Conv2D, MaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator #Data augmentation and preprocessing\n",
        "from keras.utils import to_categorical #For One-hot Encoding\n",
        "from keras.optimizers import Adam, SGD, RMSprop #For Optimizing the Neural Network\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 - Carga de documento a través de GDrive"
      ],
      "metadata": {
        "id": "qBAhcMKK6duT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDc0YgyajtFU",
        "outputId": "1b107165-b07c-4fcb-b0fd-a4e6346ff8c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir '/content/drive/MyDrive/U5M10/'\n"
      ],
      "metadata": {
        "id": "YCgCm7_nj7YW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a628fb-71d4-48a1-8fb2-fabe13e053c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/U5M10/’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip archive.zip -d '/content/drive/MyDrive/U5M10'\n",
        "\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/U5M10/archive.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/drive/MyDrive/U5M10')"
      ],
      "metadata": {
        "id": "k4RJhtnPkBdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8ZQ0OTK0m2ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "paths = os.listdir(path='/content/drive/MyDrive/U5M10/chest_xray/chest_xray')\n",
        "print(paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmM7KkifqI6r",
        "outputId": "2045d886-0f5f-4ff4-fc65-1f07fd5c168e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.DS_Store', 'test', 'train', 'val']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "train_path= pathlib.Path('/content/drive/MyDrive/U5M10/chest_xray/chest_xray/train')\n",
        "valid_path = pathlib.Path('/content/drive/MyDrive/U5M10/chest_xray/chest_xray/val')\n",
        "test_path = pathlib.Path('/content/drive/MyDrive/U5M10/chest_xray/chest_xray/test')\n"
      ],
      "metadata": {
        "id": "ARxwdVA6mugl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Comprobamos que las carpetas se hayan cargado correctamente con todas sus imágenes\n",
        "train_size = len(list(train_path.glob(\"*/*\")))\n",
        "valid_size = len(list(valid_path.glob(\"*/*\")))\n",
        "test_size = len(list(test_path.glob(\"*/*\")))\n",
        "\n",
        "print(f\"Total images in train set\\t: {train_size}\")\n",
        "print(f\"Total images in valid set\\t: {valid_size}\")\n",
        "print(f\"Total imags in test set\\t\\t: {test_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayp9vLaknUVl",
        "outputId": "16cea607-f21f-4b55-f2bb-ea29ac97b680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images in train set\t: 5218\n",
            "Total images in valid set\t: 18\n",
            "Total imags in test set\t\t: 624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n"
      ],
      "metadata": {
        "id": "NqJnvalNquZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procesamiento de imágenes para evitar overfitting"
      ],
      "metadata": {
        "id": "ncIA-vTH7Z8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255, # escalamos y obtener valores entre 0 y 1\n",
        "        shear_range=0.2, # 20% angulo de rotación\n",
        "        zoom_range=0.2, # data augmentation en un 20%, usa intervalo (0.8,1.2)\n",
        "        rotation_range=20,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_dataset = train_datagen.flow_from_directory('/content/drive/MyDrive/U5M10/chest_xray/chest_xray/train',#carga de fotos\n",
        "                                                    target_size=(64, 64),#reescalamos las fotos a la misma medida\n",
        "                                                    batch_size=32, #número de imágenes a procesar, para cada 32 fotos se revisarán los pesos\n",
        "                                                    class_mode='binary') #porque solo tenemos dos tipos de fotos\n",
        "\n",
        "testing_dataset = test_datagen.flow_from_directory('/content/drive/MyDrive/U5M10/chest_xray/chest_xray/test',\n",
        "                                                target_size=(64, 64),\n",
        "                                                batch_size=32,\n",
        "                                                class_mode='binary')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzOsO6patoyC",
        "outputId": "9e720534-3cde-4732-b78c-bbf00e91940b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 - Construir el modelo de CNN"
      ],
      "metadata": {
        "id": "Naq7cCNs8brt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = Sequential() #Inicializamos con Sequential, para que nos cree los pesos de forma aleatoria.\n"
      ],
      "metadata": {
        "id": "aB5K89gAuEjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creamos la primera capa de convolución la función Conv2D con sus argumentos\n",
        "classifier.add(Conv2D(filters = 32, # numero de filtros a aplicar, multiplo de 2^5\n",
        "                      kernel_size = (3, 3), # tamaño del filtro\n",
        "                      input_shape = (64, 64, 3), # tamaño de entrada de las fotos y si es en color o no\n",
        "                      activation = \"relu\") # función de activación\n",
        "                      )\n"
      ],
      "metadata": {
        "id": "Q8x63EVouT7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reducción de dimensión\n",
        "classifier.add(MaxPooling2D(pool_size = (2,2)))\n"
      ],
      "metadata": {
        "id": "RBNT7Kvvu426"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Aplicamos una segunda capa convolucional más max pooling.\n",
        "classifier.add(Conv2D(filters = 64,kernel_size = (3, 3), activation = \"relu\"))\n",
        "classifier.add(MaxPooling2D(pool_size = (2,2)))"
      ],
      "metadata": {
        "id": "HlQWh-fMu8jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preparación de la entrada a la red neuronal\n",
        "classifier.add(Flatten()) #aplanamos la matriz en un tensor de una dimensión\n",
        "classifier.add(Dropout(0.5)) #aleatoriamente apagamos neuronas y así ayudamos a controlar el overfitting\n"
      ],
      "metadata": {
        "id": "FYitf57fvISF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generamos la red con el método Dense\n",
        "classifier.add(Dense(units = 128, activation = \"relu\"))\n",
        "classifier.add(Dense(units = 1, activation = \"sigmoid\")) #utilizamos sigmoid porque tenemos dos categorías (pneumenía/normal)\n"
      ],
      "metadata": {
        "id": "hwustGSXvito"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CALCULO DE LOS PARAMETROS\n",
        "\n",
        "classifier.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXdJrDafv94a",
        "outputId": "c644ae19-4ed7-4670-e743-1d5846d913c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 14, 14, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 12544)             0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 12544)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               1605760   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,625,281\n",
            "Trainable params: 1,625,281\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 - Compilar y entrenar la CNN"
      ],
      "metadata": {
        "id": "2d7E3Lxhw34o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.compile(optimizer = \"RMSprop\", \n",
        "                   loss = \"binary_crossentropy\", \n",
        "                   metrics = [\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "JrZpfrRYw1QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = classifier.fit(training_dataset,\n",
        "                        steps_per_epoch = training_dataset.n//32,#41744/32=250 (valor del batch_size),\n",
        "                        epochs=5,\n",
        "                        validation_data=testing_dataset,\n",
        "                        validation_steps=testing_dataset.n//32 #624/32=19.5\n",
        "                        )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YANC1mRNxHRa",
        "outputId": "97ce6c9a-fdf6-4008-dd14-12db13ef648e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "163/163 [==============================] - 129s 771ms/step - loss: 0.4692 - accuracy: 0.7926 - val_loss: 0.3131 - val_accuracy: 0.8602\n",
            "Epoch 2/5\n",
            "163/163 [==============================] - 117s 720ms/step - loss: 0.2999 - accuracy: 0.8754 - val_loss: 0.3886 - val_accuracy: 0.8109\n",
            "Epoch 3/5\n",
            "163/163 [==============================] - 124s 761ms/step - loss: 0.2633 - accuracy: 0.8909 - val_loss: 0.2475 - val_accuracy: 0.8980\n",
            "Epoch 4/5\n",
            " 42/163 [======>.......................] - ETA: 1:30 - loss: 0.2437 - accuracy: 0.8996"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 - Validación del modelo"
      ],
      "metadata": {
        "id": "tKKCoVUS1TQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "print('Avg. Train Accuracy: ',round(np.mean(acc),3)*100)\n",
        "print('Avg. Test Accuracy:  ',round(np.mean(val_acc),4)*100)\n",
        "print('Avg. Train Loss: ',round(np.mean(loss),3)*100)\n",
        "print('Avg. Test Loss:  ',round(np.mean(val_loss),4)*100)\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OYgqPvNc1Ik1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.save('pneumonia_normal.h5')\n"
      ],
      "metadata": {
        "id": "qOa1c1p11gOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6 - Nuevas predicciones"
      ],
      "metadata": {
        "id": "O-vZv8S21jyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import required module\n",
        "import os\n",
        "# assign directory\n",
        "directory = '/content/drive/MyDrive/U5M10/chest_xray/val/NORMAL'\n",
        " \n",
        "# iterate over files in\n",
        "# that directory\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "        print(f)\n",
        "\n",
        "        # import required module\n",
        "import os\n",
        "# assign directory\n",
        "directory = '/content/drive/MyDrive/U5M10/chest_xray/val/PNEUMONIA'\n",
        " \n",
        "# iterate over files in\n",
        "# that directory\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "        print(f)"
      ],
      "metadata": {
        "id": "hSR7L39XTw2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import keras.utils as image\n",
        "#from keras.preprocessing import image\n",
        "directory = '/content/drive/MyDrive/U5M10/chest_xray/val/NORMAL'\n",
        "\n",
        "\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "      photo=f\n",
        "    print(photo)\n",
        "    test_image = keras.utils.load_img(photo, target_size = (64, 64, 3))\n",
        "    test_image = image.img_to_array(test_image)\n",
        "    print(test_image.shape)\n",
        "    test_image1 = np.expand_dims(test_image, axis = 0)\n",
        "    print(test_image1.shape)\n",
        "    result = classifier.predict(test_image1)\n",
        "    #print(result)\n",
        "    training_dataset.class_indices\n",
        "    if result[0] > np.mean(val_acc):\n",
        "        prediction = 'PNEUMONIA'\n",
        "    else:\n",
        "        prediction = 'NORMAL'\n",
        "\n",
        "    print('La foto es de un:' ,prediction)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "rq-jumQb4zhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import keras.utils as image\n",
        "#from keras.preprocessing import image\n",
        "\n",
        "directory = '/content/drive/MyDrive/U5M10/chest_xray/val/PNEUMONIA'\n",
        "\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "      photo=f\n",
        "    print(photo)\n",
        "    test_image = keras.utils.load_img(photo, target_size = (64, 64, 3))\n",
        "    test_image = image.img_to_array(test_image)\n",
        "    print(test_image.shape)\n",
        "    test_image1 = np.expand_dims(test_image, axis = 0)\n",
        "    print(test_image1.shape)\n",
        "    result = classifier.predict(test_image1)\n",
        "    #print(result)\n",
        "    training_dataset.class_indices\n",
        "    if result[0] > np.mean(val_acc):\n",
        "        prediction = 'PNEUMONIA'\n",
        "    else:\n",
        "        prediction = 'NORMAL'\n",
        "\n",
        "    print('La foto es de un:' ,prediction)\n"
      ],
      "metadata": {
        "id": "QT4Pzel8Sg4K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}